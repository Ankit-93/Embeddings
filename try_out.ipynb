{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df513956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# --------------------------\n",
    "# 1. Prepare Tokenized Corpus\n",
    "# --------------------------\n",
    "# Each sentence is a list of tokens\n",
    "# Replace this with your own tokenized data\n",
    "data = pd.read_excel(\"MEDDRA.xlsx\", sheet_name=\"_ID2NAME\")[\"Name\"].to_list()\n",
    "tokenized_corpus = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"dogs\", \"are\", \"running\", \"outside\"],\n",
    "    [\"the\", \"sun\", \"is\", \"shining\"],\n",
    "    [\"a\", \"man\", \"is\", \"playing\", \"guitar\"],\n",
    "    [\"the\", \"dog\", \"sat\", \"by\", \"the\", \"fireplace\"],\n",
    "]\n",
    "data = [[i] for i in data]\n",
    "tokenized_corpus = data\n",
    "# --------------------------\n",
    "# 2. Train Word2Vec Model\n",
    "# --------------------------\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=100,   # size of embedding vector\n",
    "    window=5,          # context window size\n",
    "    min_count=1,       # ignore words with freq < 1\n",
    "    workers=4,         # use 4 CPU threads\n",
    "    sg=1               # 1=Skip-gram, 0=CBOW\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 3. Explore Embeddings\n",
    "# --------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f61850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Vocabulary (first 20 words): ['Acute aseptic arthritis', 'Myelopathy NEC', 'Myelopathy radiation', 'Myeloproliferative disorder', 'Myeloproliferative disorder NOS', 'Myeloproliferative disorders (excl leukaemias)', 'Myelosis erythremic', 'Myelosis non-leukaemic', 'Myelosis non-leukemic', 'Myelosis nonleukemic', 'Myelosis-non-leukaemic', 'Myelosuppression', 'Myelosuppression adult', 'Myiasis', 'Myocardial contraction decreased', 'Myocardial decompensation', 'Myocardial degeneration', 'Myocardial disorders NEC', 'Myocardial disorder', 'Myelopathy neurological']\n",
      "\n",
      "ðŸ”¹ Embedding for 'Abdominal pain' (first 10 values):\n",
      " [ 0.00143053 -0.00184931  0.00987147  0.00337296 -0.00860628 -0.00763291\n",
      " -0.00070415  0.00717602  0.00958355  0.00290156]\n",
      "\n",
      "ðŸ”¹ Similarity between 'Abdominal pain' and 'Abdominal pain generalised': -0.008138582110404968\n",
      "\n",
      "ðŸ”¹ Most similar words to 'Abdominal pain':\n",
      " [('Open fracture of base of skull with other and unspecified intracranial hemorrhage', 0.4014906883239746), ('Cardiac enzymes', 0.38107311725616455), ('Other disorders of optic nerve', 0.381022185087204), ('Toxic effect of cadmium and its compounds', 0.3808828592300415), ('Janus kinase 2 mutation', 0.37001854181289673), ('Chest tightness', 0.3698740005493164), ('Breast lump biopsy', 0.36341920495033264), ('Blood alkaline phosphatase NOS abnormal', 0.3566223978996277), ('Osmolar gap normal', 0.3559369742870331), ('Large cell immunoblastic lymphoma (Anaplastic large cell lymphoma T- and null-cell types) stage II', 0.3546484708786011)]\n"
     ]
    }
   ],
   "source": [
    "# Check if the words exist in the vocabulary\n",
    "word1 = \"Abdominal pain\"\n",
    "word2 = \"Abdominal pain generalised\"\n",
    "\n",
    "# Print vocabulary (first 20 words for brevity)\n",
    "print(\"\\nðŸ”¹ Vocabulary (first 20 words):\", model.wv.index_to_key[:20])\n",
    "\n",
    "# Print embedding safely\n",
    "if word1 in model.wv:\n",
    "    print(f\"\\nðŸ”¹ Embedding for '{word1}' (first 10 values):\\n\", model.wv[word1][:10])\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ '{word1}' not in vocabulary\")\n",
    "\n",
    "# Compute similarity safely\n",
    "if word1 in model.wv and word2 in model.wv:\n",
    "    sim = model.wv.similarity(word1, word2)\n",
    "    print(f\"\\nðŸ”¹ Similarity between '{word1}' and '{word2}': {sim}\")\n",
    "else:\n",
    "    missing = [w for w in [word1, word2] if w not in model.wv]\n",
    "    print(f\"\\nâš ï¸ Cannot compute similarity, missing words: {missing}\")\n",
    "\n",
    "# Print most similar words safely\n",
    "if word1 in model.wv:\n",
    "    print(f\"\\nðŸ”¹ Most similar words to '{word1}':\\n\", model.wv.most_similar(word1))\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Cannot find most similar words for '{word1}' because it is not in vocabulary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "598b7568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model saved as my_word2vec.model\n",
      "\n",
      "ðŸ”¹ Embedding Matrix Shape: (85392, 100)\n",
      "\n",
      "âœ… Embeddings saved as embeddings.npy and vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 4. Save Model\n",
    "# --------------------------\n",
    "model.save(\"my_word2vec.model\")\n",
    "print(\"\\nâœ… Model saved as my_word2vec.model\")\n",
    "\n",
    "# --------------------------\n",
    "# 5. Export Embedding Matrix\n",
    "# --------------------------\n",
    "words = list(model.wv.index_to_key)            # list of all words\n",
    "embeddings = np.array([model.wv[w] for w in words])  # embedding matrix\n",
    "\n",
    "print(\"\\nðŸ”¹ Embedding Matrix Shape:\", embeddings.shape)  # (vocab_size, embed_dim)\n",
    "\n",
    "# Save embeddings and vocab\n",
    "np.save(\"embeddings.npy\", embeddings)\n",
    "with open(\"vocab.txt\", \"w\") as f:\n",
    "    for w in words:\n",
    "        f.write(w + \"\\n\")\n",
    "\n",
    "print(\"\\nâœ… Embeddings saved as embeddings.npy and vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e43544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Similarity between 'Abdominal pain' and 'Abdominal pain generalised': 0.9896947145462036\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Original token list\n",
    "\n",
    "# Split each phrase into words\n",
    "sentences = [phrase[0].split() for phrase in data]\n",
    "\n",
    "# Train Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1  # skip-gram\n",
    ")\n",
    "\n",
    "def get_phrase_embedding(phrase, model):\n",
    "    words = phrase.split()\n",
    "    valid_words = [w for w in words if w in model.wv]\n",
    "    if not valid_words:\n",
    "        return None\n",
    "    # Average embeddings\n",
    "    return sum(model.wv[w] for w in valid_words) / len(valid_words)\n",
    "\n",
    "word1 = \"Abdominal pain\"\n",
    "word2 = \"Abdominal pain generalised\"\n",
    "\n",
    "emb1 = get_phrase_embedding(word1, model)\n",
    "emb2 = get_phrase_embedding(word2, model)\n",
    "\n",
    "if emb1 is not None and emb2 is not None:\n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "    similarity = dot(emb1, emb2) / (norm(emb1) * norm(emb2))\n",
    "    print(f\"\\nðŸ”¹ Similarity between '{word1}' and '{word2}': {similarity}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Cannot compute similarity, missing words in model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5853e721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Similarity between 'fever' and 'liver': 0.5502948760986328\n"
     ]
    }
   ],
   "source": [
    "word1 = \"fever\"\n",
    "word2 = \"liver\"\n",
    "\n",
    "emb1 = get_phrase_embedding(word1, model)\n",
    "emb2 = get_phrase_embedding(word2, model)\n",
    "\n",
    "if emb1 is not None and emb2 is not None:\n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "    similarity = dot(emb1, emb2) / (norm(emb1) * norm(emb2))\n",
    "    print(f\"\\nðŸ”¹ Similarity between '{word1}' and '{word2}': {similarity}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Cannot compute similarity, missing words in model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e03cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_network_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
