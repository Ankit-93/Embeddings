{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0312b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58203e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT embeddings shape: torch.Size([11, 768])\n",
      "Cosine similarity between 'Pyrexia' and 'Fever': 0.917564868927002\n"
     ]
    }
   ],
   "source": [
    "# List of MedDRA PTs\n",
    "pt_terms = [\n",
    "    \"Headache\",\n",
    "    \"Nausea\",\n",
    "    \"Chest pain\",\n",
    "    \"Rash\",\n",
    "    \"Dizziness\",\n",
    "    \"Abdominal pain\",\n",
    "    \"Pyrexia\",\n",
    "    \"Dyspnoea\",\n",
    "    \"Vomiting\",\n",
    "    \"Fatigue\",\n",
    "    \"Fever\"\n",
    "]\n",
    "\n",
    "# Tokenize PT terms\n",
    "pt_enc = tokenizer(pt_terms, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get embeddings (no grad needed for inference)\n",
    "with torch.no_grad():\n",
    "    pt_emb = model(pt_enc[\"input_ids\"], pt_enc[\"attention_mask\"])\n",
    "\n",
    "print(\"PT embeddings shape:\", pt_emb.shape)  # (10, hidden_dim)\n",
    "\n",
    "# Compute cosine similarity between two PT embeddings\n",
    "import torch.nn as nn\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "# Example: compare first and second PT term embeddings\n",
    "sim = cos(pt_emb[0], pt_emb[1])\n",
    "print(f\"Cosine similarity between '{pt_terms[6]}' and '{pt_terms[-1]}':\", sim.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed498bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PTDataset(Dataset):\n",
    "    def __init__(self, pt_terms, tokenizer):\n",
    "        self.pt_terms = pt_terms\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pt_terms)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.pt_terms[idx]\n",
    "        label = idx  # Each PT gets a unique label\n",
    "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=10, return_tensors='pt')\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}, label\n",
    "\n",
    "# Example usage:\n",
    "pt_terms = [\n",
    "    \"Headache\", \"Nausea\", \"Chest pain\", \"Rash\", \"Dizziness\",\n",
    "    \"Abdominal pain\", \"Pyrexia\", \"Dyspnoea\", \"Vomiting\", \"Fatigue\"\n",
    "]\n",
    "dataset = PTDataset(pt_terms, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a53611b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PTClassifier(nn.Module):\n",
    "    def __init__(self, encoder, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9cbb6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.5489\n",
      "Epoch 2 Loss: 1.5492\n",
      "Epoch 2 Loss: 1.5492\n",
      "Epoch 3 Loss: 0.7177\n",
      "Epoch 3 Loss: 0.7177\n",
      "Epoch 4 Loss: 1.1228\n",
      "Epoch 4 Loss: 1.1228\n",
      "Epoch 5 Loss: 0.3621\n",
      "Epoch 5 Loss: 0.3621\n",
      "Epoch 6 Loss: 0.1742\n",
      "Epoch 6 Loss: 0.1742\n",
      "Epoch 7 Loss: 0.1662\n",
      "Epoch 7 Loss: 0.1662\n",
      "Epoch 8 Loss: 0.1060\n",
      "Epoch 8 Loss: 0.1060\n",
      "Epoch 9 Loss: 0.0723\n",
      "Epoch 9 Loss: 0.0723\n",
      "Epoch 10 Loss: 0.0492\n",
      "Epoch 10 Loss: 0.0492\n",
      "Epoch 11 Loss: 0.0327\n",
      "Epoch 11 Loss: 0.0327\n",
      "Epoch 12 Loss: 0.0254\n",
      "Epoch 12 Loss: 0.0254\n",
      "Epoch 13 Loss: 0.0242\n",
      "Epoch 13 Loss: 0.0242\n",
      "Epoch 14 Loss: 0.0264\n",
      "Epoch 14 Loss: 0.0264\n",
      "Epoch 15 Loss: 0.0177\n",
      "Epoch 15 Loss: 0.0177\n",
      "Epoch 16 Loss: 0.0186\n",
      "Epoch 16 Loss: 0.0186\n",
      "Epoch 17 Loss: 0.0207\n",
      "Epoch 17 Loss: 0.0207\n",
      "Epoch 18 Loss: 0.0158\n",
      "Epoch 18 Loss: 0.0158\n",
      "Epoch 19 Loss: 0.0179\n",
      "Epoch 19 Loss: 0.0179\n",
      "Epoch 20 Loss: 0.0174\n",
      "Epoch 20 Loss: 0.0174\n",
      "Epoch 21 Loss: 0.0147\n",
      "Epoch 21 Loss: 0.0147\n",
      "Epoch 22 Loss: 0.0135\n",
      "Epoch 22 Loss: 0.0135\n",
      "Epoch 23 Loss: 0.0125\n",
      "Epoch 23 Loss: 0.0125\n",
      "Epoch 24 Loss: 0.0119\n",
      "Epoch 24 Loss: 0.0119\n",
      "Epoch 25 Loss: 0.0103\n",
      "Epoch 25 Loss: 0.0103\n",
      "Epoch 26 Loss: 0.0115\n",
      "Epoch 26 Loss: 0.0115\n",
      "Epoch 27 Loss: 0.0097\n",
      "Epoch 27 Loss: 0.0097\n",
      "Epoch 28 Loss: 0.0117\n",
      "Epoch 28 Loss: 0.0117\n",
      "Epoch 29 Loss: 0.0088\n",
      "Epoch 29 Loss: 0.0088\n",
      "Epoch 30 Loss: 0.0088\n",
      "Epoch 30 Loss: 0.0088\n",
      "Epoch 31 Loss: 0.0099\n",
      "Epoch 31 Loss: 0.0099\n",
      "Epoch 32 Loss: 0.0099\n",
      "Epoch 32 Loss: 0.0099\n",
      "Epoch 33 Loss: 0.0076\n",
      "Epoch 33 Loss: 0.0076\n",
      "Epoch 34 Loss: 0.0065\n",
      "Epoch 34 Loss: 0.0065\n",
      "Epoch 35 Loss: 0.0080\n",
      "Epoch 35 Loss: 0.0080\n",
      "Epoch 36 Loss: 0.0081\n",
      "Epoch 36 Loss: 0.0081\n",
      "Epoch 37 Loss: 0.0078\n",
      "Epoch 37 Loss: 0.0078\n",
      "Epoch 38 Loss: 0.0069\n",
      "Epoch 38 Loss: 0.0069\n",
      "Epoch 39 Loss: 0.0062\n",
      "Epoch 39 Loss: 0.0062\n",
      "Epoch 40 Loss: 0.0060\n",
      "Epoch 40 Loss: 0.0060\n",
      "Epoch 41 Loss: 0.0068\n",
      "Epoch 41 Loss: 0.0068\n",
      "Epoch 42 Loss: 0.0064\n",
      "Epoch 42 Loss: 0.0064\n",
      "Epoch 43 Loss: 0.0063\n",
      "Epoch 43 Loss: 0.0063\n",
      "Epoch 44 Loss: 0.0068\n",
      "Epoch 44 Loss: 0.0068\n",
      "Epoch 45 Loss: 0.0059\n",
      "Epoch 45 Loss: 0.0059\n",
      "Epoch 46 Loss: 0.0053\n",
      "Epoch 46 Loss: 0.0053\n",
      "Epoch 47 Loss: 0.0056\n",
      "Epoch 47 Loss: 0.0056\n",
      "Epoch 48 Loss: 0.0047\n",
      "Epoch 48 Loss: 0.0047\n",
      "Epoch 49 Loss: 0.0049\n",
      "Epoch 49 Loss: 0.0049\n",
      "Epoch 50 Loss: 0.0047\n",
      "Epoch 50 Loss: 0.0047\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(pt_terms)\n",
    "model = PTClassifier(AutoModel.from_pretrained(\"bert-base-uncased\"), num_labels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(50):\n",
    "    for batch in dataloader:\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ef50767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Headache\n",
      "Expected PT: Headache\n",
      "Predicted PT: Abdominal pain\n",
      "Cosine similarities: [0.03715775907039642, 0.03708541765809059, 0.03870367258787155, 0.0318879708647728, 0.037631742656230927, 0.04065663740038872, 0.03974926471710205, 0.04024787247180939, 0.03538265824317932, 0.03604041412472725]\n",
      "Cosine distances: [0.9628422409296036, 0.9629145823419094, 0.9612963274121284, 0.9681120291352272, 0.9623682573437691, 0.9593433625996113, 0.960250735282898, 0.9597521275281906, 0.9646173417568207, 0.9639595858752728]\n",
      "\n",
      "Input: Pain in the head\n",
      "Expected PT: Headache\n",
      "Predicted PT: Abdominal pain\n",
      "Cosine similarities: [0.03715788573026657, 0.037085533142089844, 0.038703788071870804, 0.03188808262348175, 0.03763186186552048, 0.04065677151083946, 0.039749398827552795, 0.040247999131679535, 0.035382773727178574, 0.0360405258834362]\n",
      "Cosine distances: [0.9628421142697334, 0.9629144668579102, 0.9612962119281292, 0.9681119173765182, 0.9623681381344795, 0.9593432284891605, 0.9602506011724472, 0.9597520008683205, 0.9646172262728214, 0.9639594741165638]\n",
      "\n",
      "Input: Nausee\n",
      "Expected PT: Nausea\n",
      "Predicted PT: Rash\n",
      "Cosine similarities: [-0.037157729268074036, -0.037085384130477905, -0.03870363160967827, -0.03188794478774071, -0.03763170912861824, -0.04065660387277603, -0.039749227464199066, -0.040247831493616104, -0.03538262099027634, -0.03604038432240486]\n",
      "Cosine distances: [1.037157729268074, 1.037085384130478, 1.0387036316096783, 1.0318879447877407, 1.0376317091286182, 1.040656603872776, 1.039749227464199, 1.040247831493616, 1.0353826209902763, 1.0360403843224049]\n",
      "\n",
      "Input: Complains of chest discomfort\n",
      "Expected PT: Chest pain\n",
      "Predicted PT: Rash\n",
      "Cosine similarities: [-0.03715762868523598, -0.037085287272930145, -0.03870353475213051, -0.03188786283135414, -0.03763160854578018, -0.040656495839357376, -0.03974912315607071, -0.04024772346019745, -0.03538253903388977, -0.0360402874648571]\n",
      "Cosine distances: [1.037157628685236, 1.0370852872729301, 1.0387035347521305, 1.0318878628313541, 1.0376316085457802, 1.0406564958393574, 1.0397491231560707, 1.0402477234601974, 1.0353825390338898, 1.036040287464857]\n",
      "\n",
      "Input: Patient feels tired all day\n",
      "Expected PT: Fatigue\n",
      "Predicted PT: Rash\n",
      "Cosine similarities: [-0.037157539278268814, -0.037085194140672684, -0.03870344161987305, -0.03188779205083847, -0.03763151913881302, -0.04065641388297081, -0.03974903002381325, -0.040247634053230286, -0.035382453352212906, -0.03604020178318024]\n",
      "Cosine distances: [1.0371575392782688, 1.0370851941406727, 1.038703441619873, 1.0318877920508385, 1.037631519138813, 1.0406564138829708, 1.0397490300238132, 1.0402476340532303, 1.035382453352213, 1.0360402017831802]\n",
      "\n",
      "Input: Patient is not feeling well\n",
      "Expected PT: None\n",
      "Predicted PT: Rash\n",
      "Cosine similarities: [-0.0371578149497509, -0.03708547353744507, -0.03870372846722603, -0.03188803046941757, -0.0376318022608757, -0.04065670445561409, -0.039749324321746826, -0.04024793207645416, -0.0353827103972435, -0.03604047745466232]\n",
      "Cosine distances: [1.037157814949751, 1.037085473537445, 1.038703728467226, 1.0318880304694176, 1.0376318022608757, 1.040656704455614, 1.0397493243217468, 1.0402479320764542, 1.0353827103972435, 1.0360404774546623]\n",
      "\n",
      "Input: Fever and vomiting observed\n",
      "Expected PT: ['Pyrexia', 'Vomiting']\n",
      "Predicted PT: Abdominal pain\n",
      "Cosine similarities: [0.03715779259800911, 0.03708544000983238, 0.03870369493961334, 0.03188800811767578, 0.03763177990913391, 0.040656678378582, 0.039749301970005035, 0.040247898548841476, 0.03538268432021141, 0.036040447652339935]\n",
      "Cosine distances: [0.9628422074019909, 0.9629145599901676, 0.9612963050603867, 0.9681119918823242, 0.9623682200908661, 0.959343321621418, 0.960250698029995, 0.9597521014511585, 0.9646173156797886, 0.9639595523476601]\n",
      "\n",
      "Input: No complaints\n",
      "Expected PT: None\n",
      "Predicted PT: Rash\n",
      "Cosine similarities: [-0.037157852202653885, -0.037085503339767456, -0.038703761994838715, -0.03188806027173996, -0.037631843239068985, -0.040656741708517075, -0.039749372750520706, -0.04024796932935715, -0.03538275137543678, -0.03604051098227501]\n",
      "Cosine distances: [1.0371578522026539, 1.0370855033397675, 1.0387037619948387, 1.03188806027174, 1.037631843239069, 1.040656741708517, 1.0397493727505207, 1.0402479693293571, 1.0353827513754368, 1.036040510982275]\n",
      "\n",
      "Input: SOB\n",
      "Expected PT: Dyspnoea\n",
      "Predicted PT: Rash\n",
      "Cosine similarities: [-0.03715783730149269, -0.037085480988025665, -0.038703739643096924, -0.031888045370578766, -0.0376318134367466, -0.040656719356775284, -0.03974934294819832, -0.040247950702905655, -0.03538273274898529, -0.03604048863053322]\n",
      "Cosine distances: [1.0371578373014927, 1.0370854809880257, 1.038703739643097, 1.0318880453705788, 1.0376318134367466, 1.0406567193567753, 1.0397493429481983, 1.0402479507029057, 1.0353827327489853, 1.0360404886305332]\n",
      "\n",
      "Input: Dolor de cabeza\n",
      "Expected PT: Headache\n",
      "Predicted PT: Rash\n",
      "Cosine similarities: [-0.03715779632329941, -0.037085436284542084, -0.03870370239019394, -0.03188801556825638, -0.037631772458553314, -0.040656678378582, -0.039749301970005035, -0.04024790599942207, -0.03538268804550171, -0.036040447652339935]\n",
      "Cosine distances: [1.0371577963232994, 1.037085436284542, 1.038703702390194, 1.0318880155682564, 1.0376317724585533, 1.040656678378582, 1.039749301970005, 1.040247905999422, 1.0353826880455017, 1.03604044765234]\n",
      "\n",
      "Input: SOB\n",
      "Expected PT: Dyspnoea\n",
      "Predicted PT: Rash\n",
      "Cosine similarities: [-0.03715783730149269, -0.037085480988025665, -0.038703739643096924, -0.031888045370578766, -0.0376318134367466, -0.040656719356775284, -0.03974934294819832, -0.040247950702905655, -0.03538273274898529, -0.03604048863053322]\n",
      "Cosine distances: [1.0371578373014927, 1.0370854809880257, 1.038703739643097, 1.0318880453705788, 1.0376318134367466, 1.0406567193567753, 1.0397493429481983, 1.0402479507029057, 1.0353827327489853, 1.0360404886305332]\n",
      "\n",
      "Input: Dolor de cabeza\n",
      "Expected PT: Headache\n",
      "Predicted PT: Rash\n",
      "Cosine similarities: [-0.03715779632329941, -0.037085436284542084, -0.03870370239019394, -0.03188801556825638, -0.037631772458553314, -0.040656678378582, -0.039749301970005035, -0.04024790599942207, -0.03538268804550171, -0.036040447652339935]\n",
      "Cosine distances: [1.0371577963232994, 1.037085436284542, 1.038703702390194, 1.0318880155682564, 1.0376317724585533, 1.040656678378582, 1.039749301970005, 1.040247905999422, 1.0353826880455017, 1.03604044765234]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test scenarios for MedDRA PT classification/embedding with cosine distance\n",
    "import torch.nn as nn\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "test_cases = [\n",
    "    (\"Headache\", \"Headache\"),  # Exact match\n",
    "    (\"Pain in the head\", \"Headache\"),  # Synonym/paraphrase\n",
    "    (\"Nausee\", \"Nausea\"),  # Misspelling\n",
    "    (\"Complains of chest discomfort\", \"Chest pain\"),  # Short clinical note\n",
    "    (\"Patient feels tired all day\", \"Fatigue\"),  # Unseen phrase, same meaning\n",
    "    (\"Patient is not feeling well\", None),  # Ambiguous input\n",
    "    (\"Fever and vomiting observed\", [\"Pyrexia\", \"Vomiting\"]),  # Multiple symptoms\n",
    "    (\"No complaints\", None),  # Negative case\n",
    "    (\"SOB\", \"Dyspnoea\"),  # Abbreviation\n",
    "    (\"Dolor de cabeza\", \"Headache\")  # Different language (if supported)\n",
    "]\n",
    "\n",
    "for text, expected in test_cases:\n",
    "    # Tokenize and get embedding for the test input\n",
    "    enc = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
    "        if hasattr(output, 'last_hidden_state'):\n",
    "            emb = (output.last_hidden_state * enc[\"attention_mask\"].unsqueeze(-1)).sum(1)\n",
    "            emb = emb / enc[\"attention_mask\"].sum(1, keepdim=True)\n",
    "            emb = emb.squeeze(0)\n",
    "        else:\n",
    "            emb = output.mean(dim=1).squeeze(0)\n",
    "    # Compute cosine similarity and distance to each PT embedding\n",
    "    similarities = [cos(emb, pt_emb[i]).item() for i in range(len(pt_terms))]\n",
    "    distances = [1 - s for s in similarities]\n",
    "    # Find closest PT\n",
    "    min_idx = similarities.index(max(similarities))\n",
    "    predicted_pt = pt_terms[min_idx]\n",
    "    print(f\"Input: {text}\\nExpected PT: {expected}\\nPredicted PT: {predicted_pt}\")\n",
    "    print(f\"Cosine similarities: {similarities}\")\n",
    "    print(f\"Cosine distances: {distances}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "# Example data (sentence pairs)\n",
    "sents1 = [\"The cat sits outside\",\"A cat is outdoors\" ]\n",
    "sents2 = [\"Dogs are playing in the park\", \"The stock market crashed\"]\n",
    "labels = torch.tensor([1, 0])  # first pair = similar, second = dissimilar\n",
    "\n",
    "enc1 = tokenizer(sents1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "enc2 = tokenizer(sents2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "emb1 = model(enc1[\"input_ids\"], enc1[\"attention_mask\"])\n",
    "emb2 = model(enc2[\"input_ids\"], enc2[\"attention_mask\"])\n",
    "\n",
    "loss = loss_fn(emb1, emb2, labels)\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98e4fbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0780\n",
      "Epoch 2, Loss: 0.0447\n",
      "Epoch 2, Loss: 0.0447\n",
      "Epoch 3, Loss: 0.0294\n",
      "Epoch 3, Loss: 0.0294\n",
      "Epoch 4, Loss: 0.0183\n",
      "Epoch 4, Loss: 0.0183\n",
      "Epoch 5, Loss: 0.0126\n",
      "Epoch 5, Loss: 0.0126\n",
      "Epoch 6, Loss: 0.0098\n",
      "Epoch 6, Loss: 0.0098\n",
      "Epoch 7, Loss: 0.0077\n",
      "Epoch 7, Loss: 0.0077\n",
      "Epoch 8, Loss: 0.0059\n",
      "Epoch 8, Loss: 0.0059\n",
      "Epoch 9, Loss: 0.0045\n",
      "Epoch 9, Loss: 0.0045\n",
      "Epoch 10, Loss: 0.0036\n",
      "Epoch 10, Loss: 0.0036\n"
     ]
    }
   ],
   "source": [
    "# Training loop for multiple epochs\n",
    "num_epochs = 10  # You can change this value\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    emb1 = model(enc1[\"input_ids\"], enc1[\"attention_mask\"])\n",
    "    emb2 = model(enc2[\"input_ids\"], enc2[\"attention_mask\"])\n",
    "    loss = loss_fn(emb1, emb2, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e43052d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity (test): 0.6917881965637207\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model on new sentence pairs\n",
    "# Example test sentences\n",
    "new_sents1 = [\"outside\"]\n",
    "new_sents2 = [\"indoors\"]\n",
    "\n",
    "# Tokenize\n",
    "new_enc1 = tokenizer(new_sents1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "new_enc2 = tokenizer(new_sents2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get embeddings (no grad needed for inference)\n",
    "with torch.no_grad():\n",
    "    new_emb1 = model(new_enc1[\"input_ids\"], new_enc1[\"attention_mask\"])\n",
    "    new_emb2 = model(new_enc2[\"input_ids\"], new_enc2[\"attention_mask\"])\n",
    "\n",
    "# Compute cosine similarity\n",
    "cos = nn.CosineSimilarity(dim=1)\n",
    "similarity = cos(new_emb1, new_emb2)\n",
    "print(\"Cosine similarity (test):\", similarity.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_network_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
